{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "config = {\n",
    "    'split_seed_list':[42],\n",
    "    'FOLD_LIST':[0], \n",
    "    'model_path':'./result/04',\n",
    "    'model_name':'seresnext101',\n",
    "    \n",
    "    'num_classes':1,\n",
    "    'resolution':1024, #(1024,1024),(512,512),\n",
    "    'input_resolution':320, #(320,320), #(256,256), #(512,512), #(384,384)\n",
    "    'deepsupervision':False, # always false for inference\n",
    "    'clfhead':False,\n",
    "    'clf_threshold':0.5,\n",
    "    'small_mask_threshold':0, #256*25s6*0.03, #512*512*0.03,\n",
    "    'mask_threshold':0.5,\n",
    "    'pad_size':256, #(64,64), #(256,256), #(128,128)\n",
    "    \n",
    "    'tta':4,\n",
    "    'test_batch_size':12,\n",
    "    \n",
    "    'FP16':False,\n",
    "    'num_workers':4,\n",
    "    'device':torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "\n",
    "device = config['device']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m pd\u001b[38;5;241m.\u001b[39mget_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay.max_rows\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_rows\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m300\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      9\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/pyplot.py:50\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolorbar\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rcsetup, style\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/image.py:1189\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot change colors after loading data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1186\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mset_cmap(cmap)\n\u001b[0;32m-> 1189\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPcolorImage\u001b[39;00m(AxesImage):\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m    Make a pcolor-style plot with an irregular rectangular grid.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \n\u001b[1;32m   1193\u001b[0m \u001b[38;5;124;03m    This uses a variation of the original irregular image code,\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;124;03m    and it is used by pcolorfast for the corresponding grid type.\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ax,\n\u001b[1;32m   1197\u001b[0m                  x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1198\u001b[0m                  y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1202\u001b[0m                  \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1203\u001b[0m                  ):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/artist.py:119\u001b[0m, in \u001b[0;36mArtist.__init_subclass__\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_set_signature_and_docstring\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/artist.py:147\u001b[0m, in \u001b[0;36mArtist._update_set_signature_and_docstring\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m__signature__ \u001b[38;5;241m=\u001b[39m Signature(\n\u001b[1;32m    138\u001b[0m     [Parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, Parameter\u001b[38;5;241m.\u001b[39mPOSITIONAL_OR_KEYWORD),\n\u001b[1;32m    139\u001b[0m      \u001b[38;5;241m*\u001b[39m[Parameter(prop, Parameter\u001b[38;5;241m.\u001b[39mKEYWORD_ONLY, default\u001b[38;5;241m=\u001b[39m_UNSET)\n\u001b[1;32m    140\u001b[0m        \u001b[38;5;28;01mfor\u001b[39;00m prop \u001b[38;5;129;01min\u001b[39;00m ArtistInspector(\u001b[38;5;28mcls\u001b[39m)\u001b[38;5;241m.\u001b[39mget_setters()\n\u001b[1;32m    141\u001b[0m        \u001b[38;5;28;01mif\u001b[39;00m prop \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m Artist\u001b[38;5;241m.\u001b[39m_PROPERTIES_EXCLUDED_FROM_SET]])\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m_autogenerated_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSet multiple properties at once.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupported properties are\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[43mkwdoc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/artist.py:1749\u001b[0m, in \u001b[0;36mkwdoc\u001b[0;34m(artist)\u001b[0m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;124;03mInspect an `~matplotlib.artist.Artist` class (using `.ArtistInspector`) and\u001b[39;00m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;124;03mreturn information about its settable properties and their current values.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;124;03m    use in Sphinx) if it is True.\u001b[39;00m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m ai \u001b[38;5;241m=\u001b[39m ArtistInspector(artist)\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ai\u001b[38;5;241m.\u001b[39mpprint_setters_rest(leadingspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocstring.hardcopy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[0;32m-> 1749\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProperties:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpprint_setters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleadingspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/artist.py:1509\u001b[0m, in \u001b[0;36mArtistInspector.pprint_setters\u001b[0;34m(self, prop, leadingspace)\u001b[0m\n\u001b[1;32m   1506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (pad, prop, accepts)\n\u001b[1;32m   1508\u001b[0m lines \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1509\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prop \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_setters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m   1510\u001b[0m     accepts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_valid_values(prop)\n\u001b[1;32m   1511\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maliased_name(prop)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/artist.py:1435\u001b[0m, in \u001b[0;36mArtistInspector.get_setters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1433\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo, name)\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m callable(func)\n\u001b[0;32m-> 1435\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mparameters) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m   1436\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_alias(func)):\n\u001b[1;32m   1437\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1438\u001b[0m setters\u001b[38;5;241m.\u001b[39mappend(name[\u001b[38;5;241m4\u001b[39m:])\n",
      "File \u001b[0;32m/usr/lib/python3.8/inspect.py:3105\u001b[0m, in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped)\u001b[0m\n\u001b[1;32m   3103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msignature\u001b[39m(obj, \u001b[38;5;241m*\u001b[39m, follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   3104\u001b[0m     \u001b[38;5;124;03m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_wrapped\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_wrapped\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/inspect.py:2854\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[0;34m(cls, obj, follow_wrapped)\u001b[0m\n\u001b[1;32m   2851\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2852\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_callable\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, \u001b[38;5;241m*\u001b[39m, follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   2853\u001b[0m     \u001b[38;5;124;03m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_signature_from_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigcls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2855\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mfollow_wrapper_chains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_wrapped\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/inspect.py:2304\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, sigcls)\u001b[0m\n\u001b[1;32m   2299\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mreplace(parameters\u001b[38;5;241m=\u001b[39mnew_params)\n\u001b[1;32m   2301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isfunction(obj) \u001b[38;5;129;01mor\u001b[39;00m _signature_is_functionlike(obj):\n\u001b[1;32m   2302\u001b[0m     \u001b[38;5;66;03m# If it's a pure Python function, or an object that is duck type\u001b[39;00m\n\u001b[1;32m   2303\u001b[0m     \u001b[38;5;66;03m# of a Python function (Cython functions, for instance), then:\u001b[39;00m\n\u001b[0;32m-> 2304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_signature_from_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2305\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mskip_bound_arg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_bound_arg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _signature_is_builtin(obj):\n\u001b[1;32m   2308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_builtin(sigcls, obj,\n\u001b[1;32m   2309\u001b[0m                                    skip_bound_arg\u001b[38;5;241m=\u001b[39mskip_bound_arg)\n",
      "File \u001b[0;32m/usr/lib/python3.8/inspect.py:2145\u001b[0m, in \u001b[0;36m_signature_from_function\u001b[0;34m(cls, func, skip_bound_arg)\u001b[0m\n\u001b[1;32m   2143\u001b[0m func_code \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__code__\u001b[39m\n\u001b[1;32m   2144\u001b[0m pos_count \u001b[38;5;241m=\u001b[39m func_code\u001b[38;5;241m.\u001b[39mco_argcount\n\u001b[0;32m-> 2145\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_code\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_varnames\u001b[49m\n\u001b[1;32m   2146\u001b[0m posonly_count \u001b[38;5;241m=\u001b[39m func_code\u001b[38;5;241m.\u001b[39mco_posonlyargcount\n\u001b[1;32m   2147\u001b[0m positional \u001b[38;5;241m=\u001b[39m arg_names[:pos_count]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.get_option(\"display.max_columns\")\n",
    "pd.set_option('display.max_columns', 300)\n",
    "pd.get_option(\"display.max_rows\")\n",
    "pd.set_option('display.max_rows', 300)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from os.path import join as opj\n",
    "import gc\n",
    "\n",
    "import cv2\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "\n",
    "INPUT_PATH = '../../input/hubmap-organ-segmentation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(opj(INPUT_PATH, 'train.csv'))\n",
    "sub_df = pd.read_csv(opj(INPUT_PATH, 'sample_submission.csv'))\n",
    "\n",
    "print('train_df.shape = ', train_df.shape)\n",
    "print('sub_df.shape = ', sub_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "def fix_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def elapsed_time(start_time):\n",
    "    return time.time() - start_time\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "fix_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def rle2mask(rle, shape):\n",
    "    '''\n",
    "    mask_rle: run-length as string formatted (start length)\n",
    "    shape: (height, width) of array to return \n",
    "    Returns numpy array <- 1(mask), 0(background)\n",
    "    '''\n",
    "    s = rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape, order='F')  # Needed to align to RLE direction\n",
    "\n",
    "\n",
    "def mask2rle(img, shape, small_mask_threshold):\n",
    "    '''\n",
    "    Convert mask to rle.\n",
    "    img: numpy array <- 1(mask), 0(background)\n",
    "    Returns run length as string formated\n",
    "    \n",
    "    pixels = np.array([1,1,1,0,0,1,0,1,1]) #-> rle = '1 3 6 1 8 2'\n",
    "    pixels = np.concatenate([[0], pixels, [0]]) #[0,1,1,1,0,0,1,0,1,1,0]\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1 #[ 1  4  6  7  8 10] bit change points\n",
    "    print(runs[1::2]) #[4 7 10]\n",
    "    print(runs[::2]) #[1 6 8]\n",
    "    runs[1::2] -= runs[::2]\n",
    "    print(runs) #[1 3 6 1 8 2]\n",
    "    '''\n",
    "    if img.shape != shape:\n",
    "        h,w = shape\n",
    "        img = cv2.resize(img, dsize=(w,h), interpolation=cv2.INTER_LINEAR)\n",
    "    img = img.astype(np.int8) \n",
    "    pixels = img.T.flatten()\n",
    "    #pixels = np.concatenate([[0], pixels, [0]])\n",
    "    pixels = np.pad(pixels, ((1, 1), ))\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    if runs[1::2].sum() <= small_mask_threshold:\n",
    "        return ''\n",
    "    else:\n",
    "        return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "package_dir = \"../pretrained-models.pytorch/\"\n",
    "sys.path.append(package_dir)\n",
    "import pretrainedmodels\n",
    "\n",
    "\n",
    "def conv3x3(in_channel, out_channel): #not change resolusion\n",
    "    return nn.Conv2d(in_channel,out_channel,\n",
    "                      kernel_size=3,stride=1,padding=1,dilation=1,bias=False)\n",
    "\n",
    "def conv1x1(in_channel, out_channel): #not change resolution\n",
    "    return nn.Conv2d(in_channel,out_channel,\n",
    "                      kernel_size=1,stride=1,padding=0,dilation=1,bias=False)\n",
    "\n",
    "def init_weight(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        #nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        #nn.init.xavier_normal_(m.weight, gain=1)\n",
    "        #nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "        #nn.init.orthogonal_(m.weight, gain=1)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif classname.find('Batch') != -1:\n",
    "        m.weight.data.normal_(1,0.02)\n",
    "        m.bias.data.zero_()\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.orthogonal_(m.weight, gain=1)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif classname.find('Embedding') != -1:\n",
    "        nn.init.orthogonal_(m.weight, gain=1)\n",
    "\n",
    "        \n",
    "class cSEBlock(nn.Module):\n",
    "    def __init__(self, c, feat):\n",
    "        super().__init__()\n",
    "        self.attention_fc = nn.Linear(feat,1, bias=False)\n",
    "        self.bias         = nn.Parameter(torch.zeros((1,c,1), requires_grad=True))\n",
    "        self.sigmoid      = nn.Sigmoid()\n",
    "        self.dropout      = nn.Dropout2d(0.1)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        batch,c,h,w = inputs.size()\n",
    "        x = inputs.view(batch,c,-1)\n",
    "        x = self.attention_fc(x) + self.bias\n",
    "        x = x.view(batch,c,1,1)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.dropout(x)\n",
    "        return inputs * x\n",
    "\n",
    "class sSEBlock(nn.Module):\n",
    "    def __init__(self, c, h, w):\n",
    "        super().__init__()\n",
    "        self.attention_fc = nn.Linear(c,1, bias=False).apply(init_weight)\n",
    "        self.bias         = nn.Parameter(torch.zeros((1,h,w,1), requires_grad=True))\n",
    "        self.sigmoid      = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        batch,c,h,w = inputs.size()\n",
    "        x = torch.transpose(inputs, 1,2) #(*,c,h,w)->(*,h,c,w)\n",
    "        x = torch.transpose(x, 2,3) #(*,h,c,w)->(*,h,w,c)\n",
    "        x = self.attention_fc(x) + self.bias\n",
    "        x = torch.transpose(x, 2,3) #(*,h,w,1)->(*,h,1,w)\n",
    "        x = torch.transpose(x, 1,2) #(*,h,1,w)->(*,1,h,w)\n",
    "        x = self.sigmoid(x)\n",
    "        return inputs * x\n",
    "    \n",
    "class scSEBlock(nn.Module):\n",
    "    def __init__(self, c, h, w):\n",
    "        super().__init__()\n",
    "        self.cSE = cSEBlock(c,h*w)\n",
    "        self.sSE = sSEBlock(c,h,w)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x1 = self.cSE(inputs)\n",
    "        x2 = self.sSE(inputs)\n",
    "        return x1+x2\n",
    "    \n",
    "    \n",
    "# class SpatialAttention2d(nn.Module):\n",
    "#     def __init__(self, in_channel):\n",
    "#         super().__init__()\n",
    "#         self.squeeze = conv1x1(in_channel,1).apply(init_weight)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, inputs):\n",
    "#         x = self.squeeze(inputs)\n",
    "#         x = self.sigmoid(x)\n",
    "#         return inputs * x\n",
    "    \n",
    "    \n",
    "# class GAB(nn.Module):\n",
    "#     def __init__(self, in_channel, reduction=4):\n",
    "#         super().__init__()\n",
    "#         self.global_avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "#         self.conv1 = conv1x1(in_channel, in_channel//reduction).apply(init_weight)\n",
    "#         self.conv2 = conv1x1(in_channel//reduction, in_channel).apply(init_weight)\n",
    "#         self.relu  = nn.ReLU(True)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, inputs):\n",
    "#         x = self.global_avgpool(inputs)\n",
    "#         x = self.relu(self.conv1(x))\n",
    "#         x = self.sigmoid(self.conv2(x))\n",
    "#         return inputs * x\n",
    "\n",
    "    \n",
    "# class scSEBlock2(nn.Module):\n",
    "#     def __init__(self, in_channel, reduction=4):\n",
    "#         super().__init__()\n",
    "#         self.cSE = GAB(in_channel, reduction)\n",
    "#         self.sSE = SpatialAttention2d(in_channel)\n",
    "    \n",
    "#     def forward(self, inputs):\n",
    "#         x1 = self.cSE(inputs)\n",
    "#         x2 = self.sSE(inputs)\n",
    "#         return x1+x2\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.theta    = nn.utils.spectral_norm(conv1x1(channels, channels//8)).apply(init_weight)\n",
    "        self.phi      = nn.utils.spectral_norm(conv1x1(channels, channels//8)).apply(init_weight)\n",
    "        self.g        = nn.utils.spectral_norm(conv1x1(channels, channels//2)).apply(init_weight)\n",
    "        self.o        = nn.utils.spectral_norm(conv1x1(channels//2, channels)).apply(init_weight)\n",
    "        self.gamma    = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        batch,c,h,w = inputs.size()\n",
    "        theta = self.theta(inputs) #->(*,c/8,h,w)\n",
    "        phi   = F.max_pool2d(self.phi(inputs), [2,2]) #->(*,c/8,h/2,w/2)\n",
    "        g     = F.max_pool2d(self.g(inputs), [2,2]) #->(*,c/2,h/2,w/2)\n",
    "        \n",
    "        theta = theta.view(batch, self.channels//8, -1) #->(*,c/8,h*w)\n",
    "        phi   = phi.view(batch, self.channels//8, -1) #->(*,c/8,h*w/4)\n",
    "        g     = g.view(batch, self.channels//2, -1) #->(*,c/2,h*w/4)\n",
    "        \n",
    "        beta = F.softmax(torch.bmm(theta.transpose(1,2), phi), -1) #->(*,h*w,h*w/4)\n",
    "        o    = self.o(torch.bmm(g, beta.transpose(1,2)).view(batch,self.channels//2,h,w)) #->(*,c,h,w)\n",
    "        return self.gamma*o + inputs\n",
    "    \n",
    "    \n",
    "\n",
    "class ChannelAttentionModule(nn.Module):\n",
    "    def __init__(self, in_channel, reduction):\n",
    "        super().__init__()\n",
    "        self.global_maxpool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.global_avgpool = nn.AdaptiveAvgPool2d(1) \n",
    "        self.fc = nn.Sequential(\n",
    "            conv1x1(in_channel, in_channel//reduction).apply(init_weight),\n",
    "            nn.ReLU(True),\n",
    "            conv1x1(in_channel//reduction, in_channel).apply(init_weight)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x1 = self.global_maxpool(inputs)\n",
    "        x2 = self.global_avgpool(inputs)\n",
    "        x1 = self.fc(x1)\n",
    "        x2 = self.fc(x2)\n",
    "        x  = torch.sigmoid(x1 + x2)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class SpatialAttentionModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv3x3 = conv3x3(2,1).apply(init_weight)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x1,_ = torch.max(inputs, dim=1, keepdim=True)\n",
    "        x2 = torch.mean(inputs, dim=1, keepdim=True)\n",
    "        x  = torch.cat([x1,x2], dim=1)\n",
    "        x  = self.conv3x3(x)\n",
    "        x  = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channel, reduction):\n",
    "        super().__init__()\n",
    "        self.channel_attention = ChannelAttentionModule(in_channel, reduction)\n",
    "        self.spatial_attention = SpatialAttentionModule()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = inputs * self.channel_attention(inputs)\n",
    "        x = x * self.spatial_attention(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class CenterBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.conv = conv3x3(in_channel, out_channel).apply(init_weight)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecodeBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, upsample):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_channel).apply(init_weight)\n",
    "        self.upsample = nn.Sequential()\n",
    "        if upsample:\n",
    "            self.upsample.add_module('upsample',nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        self.conv3x3_1 = conv3x3(in_channel, in_channel).apply(init_weight)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channel).apply(init_weight)\n",
    "        self.conv3x3_2 = conv3x3(in_channel, out_channel).apply(init_weight)\n",
    "        self.cbam = CBAM(out_channel, reduction=16)\n",
    "        self.conv1x1   = conv1x1(in_channel, out_channel).apply(init_weight)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x  = F.relu(self.bn1(inputs))\n",
    "        x  = self.upsample(x)\n",
    "        x  = self.conv3x3_1(x)\n",
    "        x  = self.conv3x3_2(F.relu(self.bn2(x)))\n",
    "        x  = self.cbam(x)\n",
    "        x += self.conv1x1(self.upsample(inputs)) #shortcut\n",
    "        return x\n",
    "    \n",
    "    \n",
    "#U-Net ResNet34 + CBAM + hypercolumns + deepsupervision\n",
    "class UNET_RESNET34(nn.Module):\n",
    "    def __init__(self, resolution, deepsupervision, clfhead, load_weights=True):\n",
    "        super().__init__()\n",
    "        h,w = resolution\n",
    "        self.deepsupervision = deepsupervision\n",
    "        self.clfhead = clfhead\n",
    "        \n",
    "        #encoder\n",
    "        model_name = 'resnet34' #26M\n",
    "        resnet34 = pretrainedmodels.__dict__['resnet34'](num_classes=1000,pretrained=None)\n",
    "        if load_weights:\n",
    "            resnet34.load_state_dict(torch.load(f'../../../pretrainedmodels_weight/{model_name}.pth'))\n",
    "        self.conv1   = resnet34.conv1 #(*,3,h,w)->(*,64,h/2,w/2)\n",
    "        self.bn1     = resnet34.bn1\n",
    "        self.maxpool = resnet34.maxpool #->(*,64,h/4,w/4)\n",
    "        self.layer1  = resnet34.layer1 #->(*,64,h/4,w/4) \n",
    "        self.layer2  = resnet34.layer2 #->(*,128,h/8,w/8) \n",
    "        self.layer3  = resnet34.layer3 #->(*,256,h/16,w/16) \n",
    "        self.layer4  = resnet34.layer4 #->(*,512,h/32,w/32) \n",
    "        \n",
    "        #center\n",
    "        self.center  = CenterBlock(512,512) #->(*,512,h/32,w/32) \n",
    "        \n",
    "        #decoder\n",
    "        self.decoder4 = DecodeBlock(512+512,64, upsample=True) #->(*,64,h/16,w/16) \n",
    "        self.decoder3 = DecodeBlock(64+256,64, upsample=True) #->(*,64,h/8,w/8) \n",
    "        self.decoder2 = DecodeBlock(64+128,64,  upsample=True) #->(*,64,h/4,w/4) \n",
    "        self.decoder1 = DecodeBlock(64+64,64,   upsample=True) #->(*,64,h/2,w/2)\n",
    "        self.decoder0 = DecodeBlock(64,64, upsample=True) #->(*,64,h,w) \n",
    "        \n",
    "        #upsample\n",
    "        self.upsample4 = nn.Upsample(scale_factor=16, mode='bilinear', align_corners=True)\n",
    "        self.upsample3 = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True)\n",
    "        self.upsample2 = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        #deep supervision\n",
    "        self.deep4 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep3 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep2 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep1 = conv1x1(64,1).apply(init_weight)\n",
    "        \n",
    "        #final conv\n",
    "        self.final_conv = nn.Sequential(\n",
    "            conv3x3(320,64).apply(init_weight),\n",
    "            nn.ELU(True),\n",
    "            conv1x1(64,1).apply(init_weight)\n",
    "        )\n",
    "        \n",
    "        #clf head\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.BatchNorm1d(512).apply(init_weight),\n",
    "            nn.Linear(512,512).apply(init_weight),\n",
    "            nn.ELU(True),\n",
    "            nn.BatchNorm1d(512).apply(init_weight),\n",
    "            nn.Linear(512,1).apply(init_weight)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #encoder\n",
    "        x0 = F.relu(self.bn1(self.conv1(inputs))) #->(*,64,h/2,w/2) \n",
    "        x0 = self.maxpool(x0) #->(*,64,h/4,w/4)\n",
    "        x1 = self.layer1(x0) #->(*,64,h/4,w/4)\n",
    "        x2 = self.layer2(x1) #->(*,128,h/8,w/8)\n",
    "        x3 = self.layer3(x2) #->(*,256,h/16,w/16)\n",
    "        x4 = self.layer4(x3) #->(*,512,h/32,w/32)\n",
    "        \n",
    "        #clf head\n",
    "        logits_clf = self.clf(self.avgpool(x4).squeeze(-1).squeeze(-1)) #->(*,1)\n",
    "        if config['clf_threshold'] is not None:\n",
    "            if (torch.sigmoid(logits_clf)>config['clf_threshold']).sum().item()==0:\n",
    "                bs,_,h,w = inputs.shape\n",
    "                logits = torch.zeros((bs,1,h,w))\n",
    "                if self.clfhead:\n",
    "                    if self.deepsupervision:\n",
    "                        return logits,_,_\n",
    "                    else:\n",
    "                        return logits,_\n",
    "                else:\n",
    "                    if self.deepsupervision:\n",
    "                        return logits,_\n",
    "                    else:\n",
    "                        return logits\n",
    "        \n",
    "        #center\n",
    "        y5 = self.center(x4) #->(*,512,h/32,w/32)\n",
    "        \n",
    "        #decoder\n",
    "        y4 = self.decoder4(torch.cat([x4,y5], dim=1)) #->(*,64,h/16,w/16)\n",
    "        y3 = self.decoder3(torch.cat([x3,y4], dim=1)) #->(*,64,h/8,w/8)\n",
    "        y2 = self.decoder2(torch.cat([x2,y3], dim=1)) #->(*,64,h/4,w/4)\n",
    "        y1 = self.decoder1(torch.cat([x1,y2], dim=1)) #->(*,64,h/2,w/2)\n",
    "        y0 = self.decoder0(y1) #->(*,64,h,w)\n",
    "        \n",
    "        #hypercolumns\n",
    "        y4 = self.upsample4(y4) #->(*,64,h,w)\n",
    "        y3 = self.upsample3(y3) #->(*,64,h,w)\n",
    "        y2 = self.upsample2(y2) #->(*,64,h,w)\n",
    "        y1 = self.upsample1(y1) #->(*,64,h,w)\n",
    "        hypercol = torch.cat([y0,y1,y2,y3,y4], dim=1)\n",
    "        \n",
    "        #final conv\n",
    "        logits = self.final_conv(hypercol) #->(*,1,h,w)\n",
    "        \n",
    "        #clf head\n",
    "        logits_clf = self.clf(self.avgpool(x4).squeeze(-1).squeeze(-1)) #->(*,1)\n",
    "        \n",
    "        if self.clfhead:\n",
    "            if self.deepsupervision:\n",
    "                s4 = self.deep4(y4)\n",
    "                s3 = self.deep3(y3)\n",
    "                s2 = self.deep2(y2)\n",
    "                s1 = self.deep1(y1)\n",
    "                logits_deeps = [s4,s3,s2,s1]\n",
    "                return logits, logits_deeps, logits_clf\n",
    "            else:\n",
    "                return logits, logits_clf\n",
    "        else:\n",
    "            if self.deepsupervision:\n",
    "                s4 = self.deep4(y4)\n",
    "                s3 = self.deep3(y3)\n",
    "                s2 = self.deep2(y2)\n",
    "                s1 = self.deep1(y1)\n",
    "                logits_deeps = [s4,s3,s2,s1]\n",
    "                return logits, logits_deeps\n",
    "            else:\n",
    "                return logits\n",
    "\n",
    "        \n",
    "#U-Net SeResNext50 + CBAM + hypercolumns + deepsupervision\n",
    "class UNET_SERESNEXT50(nn.Module):\n",
    "    def __init__(self, resolution, deepsupervision, clfhead, load_weights=True):\n",
    "        super().__init__()\n",
    "        h,w = resolution\n",
    "        self.deepsupervision = deepsupervision\n",
    "        self.clfhead = clfhead\n",
    "        \n",
    "        #encoder\n",
    "        model_name = 'se_resnext50_32x4d' #26M\n",
    "        seresnext50 = pretrainedmodels.__dict__[model_name](pretrained=None)\n",
    "        if load_weights:\n",
    "            seresnext50.load_state_dict(torch.load(f'../../../pretrainedmodels_weight/{model_name}.pth'))\n",
    "        \n",
    "        self.encoder0 = nn.Sequential(\n",
    "            seresnext50.layer0.conv1, #(*,3,h,w)->(*,64,h/2,w/2)\n",
    "            seresnext50.layer0.bn1,\n",
    "            seresnext50.layer0.relu1,\n",
    "        )\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            seresnext50.layer0.pool, #->(*,64,h/4,w/4)\n",
    "            seresnext50.layer1 #->(*,256,h/4,w/4)\n",
    "        )\n",
    "        self.encoder2 = seresnext50.layer2 #->(*,512,h/8,w/8)\n",
    "        self.encoder3 = seresnext50.layer3 #->(*,1024,h/16,w/16)\n",
    "        self.encoder4 = seresnext50.layer4 #->(*,2048,h/32,w/32)\n",
    "        \n",
    "        #center\n",
    "        self.center  = CenterBlock(2048,512) #->(*,512,h/32,w/32) 10,16\n",
    "        \n",
    "        #decoder\n",
    "        self.decoder4 = DecodeBlock(512+2048,64, upsample=True) #->(*,64,h/16,w/16) 20,32\n",
    "        self.decoder3 = DecodeBlock(64+1024,64, upsample=True) #->(*,64,h/8,w/8) 40,64\n",
    "        self.decoder2 = DecodeBlock(64+512,64,  upsample=True) #->(*,64,h/4,w/4) 80,128\n",
    "        self.decoder1 = DecodeBlock(64+256,64,   upsample=True) #->(*,64,h/2,w/2) 160,256\n",
    "        self.decoder0 = DecodeBlock(64,64, upsample=True) #->(*,64,h,w) 320,512\n",
    "        \n",
    "        #upsample\n",
    "        self.upsample4 = nn.Upsample(scale_factor=16, mode='bilinear', align_corners=True)\n",
    "        self.upsample3 = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True)\n",
    "        self.upsample2 = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        #deep supervision\n",
    "        self.deep4 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep3 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep2 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep1 = conv1x1(64,1).apply(init_weight)\n",
    "        \n",
    "        #final conv\n",
    "        self.final_conv = nn.Sequential(\n",
    "            conv3x3(320,64).apply(init_weight),\n",
    "            nn.ELU(True),\n",
    "            conv1x1(64,1).apply(init_weight)\n",
    "        )\n",
    "        \n",
    "        #clf head\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.BatchNorm1d(2048).apply(init_weight),\n",
    "            nn.Linear(2048,512).apply(init_weight),\n",
    "            nn.ELU(True),\n",
    "            nn.BatchNorm1d(512).apply(init_weight),\n",
    "            nn.Linear(512,1).apply(init_weight)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #encoder\n",
    "        x0 = self.encoder0(inputs) #->(*,64,h/2,w/2) 160,256\n",
    "        x1 = self.encoder1(x0) #->(*,256,h/4,w/4)\n",
    "        x2 = self.encoder2(x1) #->(*,512,h/8,w/8)\n",
    "        x3 = self.encoder3(x2) #->(*,1024,h/16,w/16)\n",
    "        x4 = self.encoder4(x3) #->(*,2048,h/32,w/32)\n",
    "        \n",
    "        #clf head\n",
    "        logits_clf = self.clf(self.avgpool(x4).squeeze(-1).squeeze(-1)) #->(*,1)\n",
    "        if config['clf_threshold'] is not None:\n",
    "            if (torch.sigmoid(logits_clf)>config['clf_threshold']).sum().item()==0:\n",
    "                bs,_,h,w = inputs.shape\n",
    "                logits = torch.zeros((bs,1,h,w))\n",
    "                if self.clfhead:\n",
    "                    if self.deepsupervision:\n",
    "                        return logits,_,_\n",
    "                    else:\n",
    "                        return logits,_\n",
    "                else:\n",
    "                    if self.deepsupervision:\n",
    "                        return logits,_\n",
    "                    else:\n",
    "                        return logits\n",
    "        \n",
    "        #center\n",
    "        y5 = self.center(x4) #->(*,320,h/32,w/32)\n",
    "        \n",
    "        #decoder\n",
    "        y4 = self.decoder4(torch.cat([x4,y5], dim=1)) #->(*,64,h/16,w/16)\n",
    "        y3 = self.decoder3(torch.cat([x3,y4], dim=1)) #->(*,64,h/8,w/8)\n",
    "        y2 = self.decoder2(torch.cat([x2,y3], dim=1)) #->(*,64,h/4,w/4)\n",
    "        y1 = self.decoder1(torch.cat([x1,y2], dim=1)) #->(*,64,h/2,w/2) 160,256\n",
    "        y0 = self.decoder0(y1) #->(*,64,h,w) 320,512\n",
    "        \n",
    "        #hypercolumns\n",
    "        y4 = self.upsample4(y4) #->(*,64,h,w)\n",
    "        y3 = self.upsample3(y3) #->(*,64,h,w)\n",
    "        y2 = self.upsample2(y2) #->(*,64,h,w)\n",
    "        y1 = self.upsample1(y1) #->(*,64,h,w)\n",
    "        hypercol = torch.cat([y0,y1,y2,y3,y4], dim=1)\n",
    "        \n",
    "        #final conv\n",
    "        logits = self.final_conv(hypercol) #->(*,4,h,w)\n",
    "        \n",
    "        #clf head\n",
    "        logits_clf = self.clf(self.avgpool(x4).squeeze(-1).squeeze(-1)) #->(*,1)\n",
    "        \n",
    "        if self.clfhead:\n",
    "            if self.deepsupervision:\n",
    "                s4 = self.deep4(y4)\n",
    "                s3 = self.deep3(y3)\n",
    "                s2 = self.deep2(y2)\n",
    "                s1 = self.deep1(y1)\n",
    "                logits_deeps = [s4,s3,s2,s1]\n",
    "                return logits, logits_deeps, logits_clf\n",
    "            else:\n",
    "                return logits, logits_clf\n",
    "        else:\n",
    "            if self.deepsupervision:\n",
    "                s4 = self.deep4(y4)\n",
    "                s3 = self.deep3(y3)\n",
    "                s2 = self.deep2(y2)\n",
    "                s1 = self.deep1(y1)\n",
    "                logits_deeps = [s4,s3,s2,s1]\n",
    "                return logits, logits_deeps\n",
    "            else:\n",
    "                return logits\n",
    "    \n",
    "\n",
    "#U-Net SeResNext101 + CBAM + hypercolumns + deepsupervision\n",
    "class UNET_SERESNEXT101(nn.Module):\n",
    "    def __init__(self, resolution, deepsupervision, clfhead, load_weights=True):\n",
    "        super().__init__()\n",
    "        h,w = resolution\n",
    "        self.deepsupervision = deepsupervision\n",
    "        self.clfhead = clfhead\n",
    "        \n",
    "        #encoder\n",
    "        model_name = 'se_resnext101_32x4d'\n",
    "        seresnext101 = pretrainedmodels.__dict__[model_name](pretrained=None)\n",
    "        if load_weights:\n",
    "            seresnext101.load_state_dict(torch.load(f'../../../pretrainedmodels_weight/{model_name}.pth'))\n",
    "        \n",
    "        self.encoder0 = nn.Sequential(\n",
    "            seresnext101.layer0.conv1, #(*,3,h,w)->(*,64,h/2,w/2)\n",
    "            seresnext101.layer0.bn1,\n",
    "            seresnext101.layer0.relu1,\n",
    "        )\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            seresnext101.layer0.pool, #->(*,64,h/4,w/4)\n",
    "            seresnext101.layer1 #->(*,256,h/4,w/4)\n",
    "        )\n",
    "        self.encoder2 = seresnext101.layer2 #->(*,512,h/8,w/8)\n",
    "        self.encoder3 = seresnext101.layer3 #->(*,1024,h/16,w/16)\n",
    "        self.encoder4 = seresnext101.layer4 #->(*,2048,h/32,w/32)\n",
    "        \n",
    "        #center\n",
    "        self.center  = CenterBlock(2048,512) #->(*,512,h/32,w/32)\n",
    "        \n",
    "        #decoder\n",
    "        self.decoder4 = DecodeBlock(512+2048,64, upsample=True) #->(*,64,h/16,w/16)\n",
    "        self.decoder3 = DecodeBlock(64+1024,64, upsample=True) #->(*,64,h/8,w/8)\n",
    "        self.decoder2 = DecodeBlock(64+512,64,  upsample=True) #->(*,64,h/4,w/4) \n",
    "        self.decoder1 = DecodeBlock(64+256,64,   upsample=True) #->(*,64,h/2,w/2) \n",
    "        self.decoder0 = DecodeBlock(64,64, upsample=True) #->(*,64,h,w) \n",
    "        \n",
    "        #upsample\n",
    "        self.upsample4 = nn.Upsample(scale_factor=16, mode='bilinear', align_corners=True)\n",
    "        self.upsample3 = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True)\n",
    "        self.upsample2 = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        #deep supervision\n",
    "        self.deep4 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep3 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep2 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep1 = conv1x1(64,1).apply(init_weight)\n",
    "        \n",
    "        #final conv\n",
    "        self.final_conv = nn.Sequential(\n",
    "            conv3x3(320,64).apply(init_weight),\n",
    "            nn.ELU(True),\n",
    "            conv1x1(64,1).apply(init_weight)\n",
    "        )\n",
    "        \n",
    "        #clf head\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.BatchNorm1d(2048).apply(init_weight),\n",
    "            nn.Linear(2048,512).apply(init_weight),\n",
    "            nn.ELU(True),\n",
    "            nn.BatchNorm1d(512).apply(init_weight),\n",
    "            nn.Linear(512,1).apply(init_weight)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #encoder\n",
    "        x0 = self.encoder0(inputs) #->(*,64,h/2,w/2)\n",
    "        x1 = self.encoder1(x0) #->(*,256,h/4,w/4)\n",
    "        x2 = self.encoder2(x1) #->(*,512,h/8,w/8)\n",
    "        x3 = self.encoder3(x2) #->(*,1024,h/16,w/16)\n",
    "        x4 = self.encoder4(x3) #->(*,2048,h/32,w/32)\n",
    "        \n",
    "        #clf head\n",
    "        logits_clf = self.clf(self.avgpool(x4).squeeze(-1).squeeze(-1)) #->(*,1)\n",
    "        if config['clf_threshold'] is not None:\n",
    "            if (torch.sigmoid(logits_clf)>config['clf_threshold']).sum().item()==0:\n",
    "                bs,_,h,w = inputs.shape\n",
    "                logits = torch.zeros((bs,1,h,w))\n",
    "                if self.clfhead:\n",
    "                    if self.deepsupervision:\n",
    "                        return logits,_,_\n",
    "                    else:\n",
    "                        return logits,_\n",
    "                else:\n",
    "                    if self.deepsupervision:\n",
    "                        return logits,_\n",
    "                    else:\n",
    "                        return logits\n",
    "        \n",
    "        #center\n",
    "        y5 = self.center(x4) #->(*,320,h/32,w/32)\n",
    "        \n",
    "        #decoder\n",
    "        y4 = self.decoder4(torch.cat([x4,y5], dim=1)) #->(*,64,h/16,w/16)\n",
    "        y3 = self.decoder3(torch.cat([x3,y4], dim=1)) #->(*,64,h/8,w/8)\n",
    "        y2 = self.decoder2(torch.cat([x2,y3], dim=1)) #->(*,64,h/4,w/4)\n",
    "        y1 = self.decoder1(torch.cat([x1,y2], dim=1)) #->(*,64,h/2,w/2) \n",
    "        y0 = self.decoder0(y1) #->(*,64,h,w)\n",
    "        \n",
    "        #hypercolumns\n",
    "        y4 = self.upsample4(y4) #->(*,64,h,w)\n",
    "        y3 = self.upsample3(y3) #->(*,64,h,w)\n",
    "        y2 = self.upsample2(y2) #->(*,64,h,w)\n",
    "        y1 = self.upsample1(y1) #->(*,64,h,w)\n",
    "        hypercol = torch.cat([y0,y1,y2,y3,y4], dim=1)\n",
    "        \n",
    "        #final conv\n",
    "        logits = self.final_conv(hypercol) #->(*,1,h,w)\n",
    "        \n",
    "        #clf head\n",
    "        logits_clf = self.clf(self.avgpool(x4).squeeze(-1).squeeze(-1)) #->(*,1)\n",
    "        \n",
    "        if self.clfhead:\n",
    "            if self.deepsupervision:\n",
    "                s4 = self.deep4(y4)\n",
    "                s3 = self.deep3(y3)\n",
    "                s2 = self.deep2(y2)\n",
    "                s1 = self.deep1(y1)\n",
    "                logits_deeps = [s4,s3,s2,s1]\n",
    "                return logits, logits_deeps, logits_clf\n",
    "            else:\n",
    "                return logits, logits_clf\n",
    "        else:\n",
    "            if self.deepsupervision:\n",
    "                s4 = self.deep4(y4)\n",
    "                s3 = self.deep3(y3)\n",
    "                s2 = self.deep2(y2)\n",
    "                s1 = self.deep1(y1)\n",
    "                logits_deeps = [s4,s3,s2,s1]\n",
    "                return logits, logits_deeps\n",
    "            else:\n",
    "                return logits    \n",
    "\n",
    "    \n",
    "def build_model(resolution, deepsupervision, clfhead, load_weights):\n",
    "    model_name = config['model_name']\n",
    "    if model_name=='resnet34':\n",
    "        model = UNET_RESNET34(resolution, deepsupervision, clfhead, load_weights)\n",
    "    elif model_name=='seresnext50':\n",
    "        model = UNET_SERESNEXT50(resolution, deepsupervision, clfhead, load_weights)\n",
    "    elif model_name=='seresnext101':\n",
    "        model = UNET_SERESNEXT101(resolution, deepsupervision, clfhead, load_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models import build_model\n",
    "\n",
    "LOAD_LOCAL_WEIGHT_PATH_LIST = {}\n",
    "for seed in config['split_seed_list']:\n",
    "    LOAD_LOCAL_WEIGHT_PATH_LIST[seed] = []\n",
    "    for fold in config['FOLD_LIST']:\n",
    "        LOAD_LOCAL_WEIGHT_PATH_LIST[seed].append(opj(config['model_path'],f'model_seed{seed}_fold{fold}_bestscore.pth'))\n",
    "        #LOAD_LOCAL_WEIGHT_PATH_LIST[seed].append(opj(config['model_path'],f'model_seed{seed}_fold{fold}_swa.pth'))\n",
    "\n",
    "model_list = {}\n",
    "for seed in config['split_seed_list']:\n",
    "    model_list[seed] = []\n",
    "    for path in LOAD_LOCAL_WEIGHT_PATH_LIST[seed]:\n",
    "        print(\"Loading weights from %s\" % path)\n",
    "        \n",
    "        model = build_model(resolution=(None,None), #config['resolution'], \n",
    "                            deepsupervision=config['deepsupervision'], \n",
    "                            clfhead=config['clfhead'],\n",
    "                            load_weights=False).to(device)\n",
    "        \n",
    "        model.load_state_dict(torch.load(path))\n",
    "        model.eval()\n",
    "        model_list[seed].append(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from albumentations import (Compose, HorizontalFlip, VerticalFlip, Rotate, RandomRotate90,\n",
    "                            ShiftScaleRotate, ElasticTransform,\n",
    "                            GridDistortion, RandomSizedCrop, RandomCrop, CenterCrop,\n",
    "                            RandomBrightnessContrast, HueSaturationValue, IAASharpen,\n",
    "                            RandomGamma, RandomBrightness, RandomBrightnessContrast,\n",
    "                            GaussianBlur,CLAHE,\n",
    "                            Cutout, CoarseDropout, GaussNoise, ChannelShuffle, ToGray, OpticalDistortion,\n",
    "                            Normalize, OneOf, NoOp)\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "#from get_config import *\n",
    "#config = get_config()\n",
    "\n",
    "MEAN = np.array([0.772, 0.746, 0.764])\n",
    "STD  = np.array([0.247, 0.262, 0.258])\n",
    "\n",
    "def get_transforms_test():\n",
    "    transforms = Compose([\n",
    "        Normalize(mean=(MEAN[0], MEAN[1], MEAN[2]), \n",
    "                  std=(STD[0], STD[1], STD[2])),\n",
    "        ToTensorV2(),\n",
    "    ] )\n",
    "    return transforms\n",
    "\n",
    "def denormalize(z, mean=MEAN.reshape(-1,1,1), std=STD.reshape(-1,1,1)):\n",
    "    return std*z + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HuBMAPDataset(Dataset):\n",
    "    def __init__(self, idx, df):\n",
    "        super().__init__()\n",
    "        filename = str(df.loc[idx, 'id'])+'.tiff'\n",
    "        path = opj(INPUT_PATH,'train_images',filename)\n",
    "        self.data = rasterio.open(path)\n",
    "        if self.data.count != 3:\n",
    "            subdatasets = self.data.subdatasets\n",
    "            self.layers = []\n",
    "            if len(subdatasets) > 0:\n",
    "                for i,subdataset in enumerate(subdatasets,0):\n",
    "                    self.layers.append(rasterio.open(subdataset))\n",
    "        self.h, self.w = self.data.height, self.data.width\n",
    "        self.input_sz = config['input_resolution']\n",
    "        self.sz = config['resolution']\n",
    "        self.pad_sz = config['pad_size'] # add to each input tile\n",
    "        self.pred_sz = self.sz - 2*self.pad_sz\n",
    "        self.pad_h = self.pred_sz - self.h % self.pred_sz # add to whole slide\n",
    "        self.pad_w = self.pred_sz - self.w % self.pred_sz # add to whole slide\n",
    "        self.num_h = (self.h + self.pad_h) // self.pred_sz\n",
    "        self.num_w = (self.w + self.pad_w) // self.pred_sz\n",
    "        self.transforms = get_transforms_test()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_h * self.num_w\n",
    "    \n",
    "    def __getitem__(self, idx): # idx = i_h * self.num_w + i_w\n",
    "        # prepare coordinates for rasterio\n",
    "        i_h = idx // self.num_w\n",
    "        i_w = idx % self.num_w\n",
    "        y = i_h*self.pred_sz \n",
    "        x = i_w*self.pred_sz\n",
    "        py0,py1 = max(0,y), min(y+self.pred_sz, self.h)\n",
    "        px0,px1 = max(0,x), min(x+self.pred_sz, self.w)\n",
    "        \n",
    "        # padding coordinate for rasterio\n",
    "        qy0,qy1 = max(0,y-self.pad_sz), min(y+self.pred_sz+self.pad_sz, self.h)\n",
    "        qx0,qx1 = max(0,x-self.pad_sz), min(x+self.pred_sz+self.pad_sz, self.w)\n",
    "        \n",
    "        # placeholder for input tile (before resize)\n",
    "        img = np.zeros((self.sz,self.sz,3), np.uint8)\n",
    "        \n",
    "        # replace the value\n",
    "        if self.data.count == 3:\n",
    "            img[0:qy1-qy0, 0:qx1-qx0] =\\\n",
    "                np.moveaxis(self.data.read([1,2,3], window=Window.from_slices((qy0,qy1),(qx0,qx1))), 0,-1)\n",
    "        else:\n",
    "            for i,layer in enumerate(self.layers):\n",
    "                img[0:qy1-qy0, 0:qx1-qx0, i] =\\\n",
    "                    layer.read(1,window=Window.from_slices((qy0,qy1),(qx0,qx1)))\n",
    "        if self.sz != self.input_sz:\n",
    "            img = cv2.resize(img, (self.input_sz, self.input_sz), interpolation=cv2.INTER_AREA)\n",
    "        img = self.transforms(image=img)['image'] # to normalized tensor\n",
    "        return {'img':img, 'p':[py0,py1,px0,px1], 'q':[qy0,qy1,qx0,qx1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "import math\n",
    "\n",
    "\n",
    "def my_collate_fn(batch):\n",
    "    img = []\n",
    "    p = []\n",
    "    q = []\n",
    "    for sample in batch:\n",
    "        img.append(sample['img'])\n",
    "        p.append(sample['p'])\n",
    "        q.append(sample['q'])\n",
    "    img = torch.stack(img)\n",
    "    return {'img':img, 'p':p, 'q':q}\n",
    "\n",
    "\n",
    "seed = 42\n",
    "\n",
    "def get_pred_mask(idx, df, model_list):\n",
    "    ds = HuBMAPDataset(idx, df)\n",
    "    #rasterio cannot be used with multiple workers\n",
    "    dl = DataLoader(ds,batch_size=config['test_batch_size'],\n",
    "                    num_workers=0,shuffle=False,pin_memory=True,\n",
    "                    collate_fn=my_collate_fn) \n",
    "    \n",
    "    pred_mask = np.zeros((len(ds),ds.pred_sz,ds.pred_sz), dtype=np.uint8)\n",
    "    \n",
    "    i_data = 0\n",
    "    for data in tqdm(dl):\n",
    "        bs = data['img'].shape[0]\n",
    "        img_patch = data['img'] # (bs,3,input_res,input_res)\n",
    "        pred_mask_float = 0\n",
    "        for model in model_list[seed]:\n",
    "            with torch.no_grad():\n",
    "                if config['tta']>0:\n",
    "                    pred_mask_float += torch.sigmoid(model(img_patch.to(device, torch.float32, non_blocking=True))).detach().cpu().numpy()[:,0,:,:] #.squeeze()\n",
    "                if config['tta']>1:\n",
    "                    # h-flip\n",
    "                    _pred_mask_float = torch.sigmoid(model(img_patch.flip([-1]).to(device, torch.float32, non_blocking=True))).detach().cpu().numpy()[:,0,:,:] #.squeeze()\n",
    "                    pred_mask_float += _pred_mask_float[:,:,::-1]\n",
    "                if config['tta']>2:\n",
    "                    # v-flip\n",
    "                    _pred_mask_float = torch.sigmoid(model(img_patch.flip([-2]).to(device, torch.float32, non_blocking=True))).detach().cpu().numpy()[:,0,:,:] #.squeeze()\n",
    "                    pred_mask_float += _pred_mask_float[:,::-1,:]\n",
    "                if config['tta']>3:\n",
    "                    # h-v-flip\n",
    "                    _pred_mask_float = torch.sigmoid(model(img_patch.flip([-1,-2]).to(device, torch.float32, non_blocking=True))).detach().cpu().numpy()[:,0,:,:] #.squeeze()\n",
    "                    pred_mask_float += _pred_mask_float[:,::-1,::-1]\n",
    "        pred_mask_float = pred_mask_float / min(config['tta'],4) / len(model_list[seed]) # (bs,input_res,input_res)\n",
    "        \n",
    "        # resize\n",
    "        pred_mask_float = np.vstack([cv2.resize(_mask.astype(np.float32), (ds.sz,ds.sz))[None] for _mask in pred_mask_float])\n",
    "        \n",
    "        # float to uint8\n",
    "        pred_mask_int = (pred_mask_float>config['mask_threshold']).astype(np.uint8)\n",
    "        \n",
    "        # replace the values\n",
    "        for j in range(bs):\n",
    "            py0,py1,px0,px1 = data['p'][j]\n",
    "            qy0,qy1,qx0,qx1 = data['q'][j]\n",
    "            pred_mask[i_data+j,0:py1-py0, 0:px1-px0] = pred_mask_int[j, py0-qy0:py1-qy0, px0-qx0:px1-qx0] # (pred_sz,pred_sz)\n",
    "        i_data += bs\n",
    "    \n",
    "    pred_mask = pred_mask.reshape(ds.num_h*ds.num_w, ds.pred_sz, ds.pred_sz).reshape(ds.num_h, ds.num_w, ds.pred_sz, ds.pred_sz)\n",
    "    pred_mask = pred_mask.transpose(0,2,1,3).reshape(ds.num_h*ds.pred_sz, ds.num_w*ds.pred_sz)\n",
    "    pred_mask = pred_mask[:ds.h,:ds.w] # back to the original slide size\n",
    "    non_zero_ratio = (pred_mask).sum() / (ds.h*ds.w)\n",
    "    print('non_zero_ratio = {:.4f}'.format(non_zero_ratio))\n",
    "    return pred_mask,ds.h,ds.w\n",
    "\n",
    "def get_rle(y_preds, h,w):\n",
    "    rle = mask2rle(y_preds, shape=(h,w), small_mask_threshold=config['small_mask_threshold'])\n",
    "    return rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for idx in range(len(sub_df)): \n",
    "    print('idx = ', idx)\n",
    "    pred_mask,h,w = get_pred_mask(idx, sub_df, model_list)\n",
    "    rle = get_rle(pred_mask,h,w)\n",
    "    sub_df.loc[idx,'rle'] = rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tifffile as tiff \n",
    "\n",
    "img = tiff.imread('/home/andrew/Documents/HuBMAP/input/hubmap-organ-segmentation/test_images/10078.tiff')\n",
    "mask = rle2mask(sub_df.loc[0, 'rle'], img.shape[:2])\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img)\n",
    "plt.imshow(mask, cmap='coolwarm', alpha=0.5)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
